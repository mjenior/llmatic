{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Initialize the handler with default or provided values.\n        \"\"\""}, {"role": "assistant", "content": "def __init__(\n        self,\n        logging=True,\n        verbose=True,\n        silent=False,\n        refine=False,\n        glyph=False,\n        chain_of_thought=False,\n        save_code=False,\n        scan_dirs=False,\n        new_chat=False,\n        role=\"assistant\",\n        seed=\"t634e``R75T86979UYIUHGVCXZ\",\n        iterations=1,\n        temperature=0.7,\n        top_p=1.0,\n        dimensions=\"NA\",\n        quality=\"NA\",\n        stage=\"normal\",\n        message_limit=20,\n        **kwargs,\n    ):\n        \n        self.logging = logging\n        self.verbose = verbose\n        self.silent = silent\n        self.refine_prompt = refine\n        self.glyph_prompt = glyph\n        self.chain_of_thought = chain_of_thought\n        self.save_code = save_code\n        self.scan_dirs = scan_dirs\n        self.new_chat = new_chat\n        self.role = role\n        self.seed = seed\n        self.iterations = iterations\n        self.temperature = temperature\n        self.top_p = top_p\n        self.dimensions = dimensions\n        self.quality = quality\n        self.stage = stage\n        self.message_limit = message_limit\n\n        # Check user input types\n        self._validate_types()\n\n        # Agent-specific chat params\n        global chat\n        self.chat_id = chat.id\n        chat.message_limit = message_limit\n        if self.new_chat == True:\n            self.start_new_chat()\n\n        # Update token counters\n        global total_tokens\n        self.cost = {\"prompt\": 0.0, \"completion\": 0.0}\n        self.tokens = {\"prompt\": 0, \"completion\": 0}\n        if self.model not in total_tokens.keys():\n            total_tokens[self.model] = {\"prompt\": 0, \"completion\": 0}\n\n        # Validate specific hyperparams\n        self._validate_model_selection(self.model, kwargs[\"valid_models\"])\n        self.stage = self.stage if self.stage == \"refine_only\" else \"normal\"\n        self.seed = (\n            self.seed\n            if isinstance(self.seed, int)\n            else utils.string_to_binary(self.seed)\n        )\n        self.temperature, self.top_p = utils.validate_probability_params(\n            self.temperature, self.top_p\n        )\n\n        # Validate user inputs\n        self._prepare_system_role(role)\n        if self.model in [\"dall-e-2\", \"dall-e-3\"]:\n            self._validate_image_params(dimensions, quality)\n        self._create_new_agent(interpreter=self.save_code)\n\n        # Initialize reporting and related vars\n        self.timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.prefix = f\"{self.label}.{self.model.replace('-', '_')}.{self.timestamp}\"\n        if self.logging:\n            self.log_file = utils.setup_logging(self.prefix)\n        self._log_and_print(self.status(), False, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def __init__(\n        self,\n        logging=True,\n        verbose=True,\n        silent=False,\n        refine=False,\n        glyph=False,\n        chain_of_thought=False,\n        save_code=False,\n        scan_dirs=False,\n        new_chat=False,\n        role=\"assistant\",\n        seed=\"t634e``R75T86979UYIUHGVCXZ\",\n        iterations=1,\n        temperature=0.7,\n        top_p=1.0,\n        dimensions=\"NA\",\n        quality=\"NA\",\n        stage=\"normal\",\n        message_limit=20,\n        **kwargs,\n    ):\n        \n        self.logging = logging\n        self.verbose = verbose\n        self.silent = silent\n        self.refine_prompt = refine\n        self.glyph_prompt = glyph\n        self.chain_of_thought = chain_of_thought\n        self.save_code = save_code\n        self.scan_dirs = scan_dirs\n        self.new_chat = new_chat\n        self.role = role\n        self.seed = seed\n        self.iterations = iterations\n        self.temperature = temperature\n        self.top_p = top_p\n        self.dimensions = dimensions\n        self.quality = quality\n        self.stage = stage\n        self.message_limit = message_limit\n\n        # Check user input types\n        self._validate_types()\n\n        # Agent-specific chat params\n        global chat\n        self.chat_id = chat.id\n        chat.message_limit = message_limit\n        if self.new_chat == True:\n            self.start_new_chat()\n\n        # Update token counters\n        global total_tokens\n        self.cost = {\"prompt\": 0.0, \"completion\": 0.0}\n        self.tokens = {\"prompt\": 0, \"completion\": 0}\n        if self.model not in total_tokens.keys():\n            total_tokens[self.model] = {\"prompt\": 0, \"completion\": 0}\n\n        # Validate specific hyperparams\n        self._validate_model_selection(self.model, kwargs[\"valid_models\"])\n        self.stage = self.stage if self.stage == \"refine_only\" else \"normal\"\n        self.seed = (\n            self.seed\n            if isinstance(self.seed, int)\n            else utils.string_to_binary(self.seed)\n        )\n        self.temperature, self.top_p = utils.validate_probability_params(\n            self.temperature, self.top_p\n        )\n\n        # Validate user inputs\n        self._prepare_system_role(role)\n        if self.model in [\"dall-e-2\", \"dall-e-3\"]:\n            self._validate_image_params(dimensions, quality)\n        self._create_new_agent(interpreter=self.save_code)\n\n        # Initialize reporting and related vars\n        self.timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.prefix = f\"{self.label}.{self.model.replace('-', '_')}.{self.timestamp}\"\n        if self.logging:\n            self.log_file = utils.setup_logging(self.prefix)\n        self._log_and_print(self.status(), False, self.logging)"}, {"role": "assistant", "content": "\"\"\"\n        Initialize the handler with default or provided values.\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Validates the types of the instance attributes for CreateAgent.\n\n        Raises:\n            TypeError: If any attribute has an incorrect type.\n            ValueError: If any integer attribute is not positive.\n        \"\"\""}, {"role": "assistant", "content": "def _validate_types(self):\n        \n        expected_types = {\n            \"model\": str,\n            \"logging\": bool,\n            \"verbose\": bool,\n            \"silent\": bool,\n            \"refine_prompt\": bool,\n            \"glyph_prompt\": bool,\n            \"chain_of_thought\": bool,\n            \"save_code\": bool,\n            \"scan_dirs\": bool,\n            \"new_chat\": bool,\n            \"model\": str,\n            \"role\": str,\n            \"seed\": (int, str),  # seed can be either int or str\n            \"iterations\": int,\n            \"temperature\": float,\n            \"top_p\": float,\n            \"dimensions\": str,\n            \"quality\": str,\n            \"stage\": str,\n            \"message_limit\": int,\n        }\n\n        for attr_name, expected_type in expected_types.items():\n            value = getattr(self, attr_name, None)  # Get the attribute value from self\n            if isinstance(expected_type, tuple):\n                # Check if value matches any expected type in the tuple\n                if not isinstance(value, expected_type):\n                    raise TypeError(\n                        f\"Expected type for {attr_name} is {expected_type}, got {type(value).__name__}\"\n                    )\n            else:\n                # Check if value matches the expected type\n                if not isinstance(value, expected_type):\n                    raise TypeError(\n                        f\"Expected type for {attr_name} is {expected_type}, got {type(value).__name__}\"\n                    )\n\n            # Check if integer-type values are positive\n            if expected_type == int and value <= 0:\n                raise ValueError(f\"{attr_name} must be a positive integer, got {value}\")"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _validate_types(self):\n        \n        expected_types = {\n            \"model\": str,\n            \"logging\": bool,\n            \"verbose\": bool,\n            \"silent\": bool,\n            \"refine_prompt\": bool,\n            \"glyph_prompt\": bool,\n            \"chain_of_thought\": bool,\n            \"save_code\": bool,\n            \"scan_dirs\": bool,\n            \"new_chat\": bool,\n            \"model\": str,\n            \"role\": str,\n            \"seed\": (int, str),  # seed can be either int or str\n            \"iterations\": int,\n            \"temperature\": float,\n            \"top_p\": float,\n            \"dimensions\": str,\n            \"quality\": str,\n            \"stage\": str,\n            \"message_limit\": int,\n        }\n\n        for attr_name, expected_type in expected_types.items():\n            value = getattr(self, attr_name, None)  # Get the attribute value from self\n            if isinstance(expected_type, tuple):\n                # Check if value matches any expected type in the tuple\n                if not isinstance(value, expected_type):\n                    raise TypeError(\n                        f\"Expected type for {attr_name} is {expected_type}, got {type(value).__name__}\"\n                    )\n            else:\n                # Check if value matches the expected type\n                if not isinstance(value, expected_type):\n                    raise TypeError(\n                        f\"Expected type for {attr_name} is {expected_type}, got {type(value).__name__}\"\n                    )\n\n            # Check if integer-type values are positive\n            if expected_type == int and value <= 0:\n                raise ValueError(f\"{attr_name} must be a positive integer, got {value}\")"}, {"role": "assistant", "content": "\"\"\"\n        Validates the types of the instance attributes for CreateAgent.\n\n        Raises:\n            TypeError: If any attribute has an incorrect type.\n            ValueError: If any integer attribute is not positive.\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Prepares the query, including prompt modifications and image handling.\n        \"\"\""}, {"role": "assistant", "content": "def _prepare_query_text(self, prompt_text):\n        \n        self.prompt = prompt_text\n\n        # Identifies files to be read in\n        files = utils.find_existing_files(prompt_text)\n        for f in files:\n            self.prompt += \"\\n\\n\" + utils.read_file_contents(f)\n        if self.scan_dirs == True:\n            paths = utils.find_existing_paths(prompt_text)\n            for d in paths:\n                self.prompt += \"\\n\\n\" + utils.scan_directory(d)\n\n        # Refine prompt if required\n        if self.refine_prompt or self.glyph_prompt:\n            self.prompt = self._refine_user_prompt(self.prompt)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _prepare_query_text(self, prompt_text):\n        \n        self.prompt = prompt_text\n\n        # Identifies files to be read in\n        files = utils.find_existing_files(prompt_text)\n        for f in files:\n            self.prompt += \"\\n\\n\" + utils.read_file_contents(f)\n        if self.scan_dirs == True:\n            paths = utils.find_existing_paths(prompt_text)\n            for d in paths:\n                self.prompt += \"\\n\\n\" + utils.scan_directory(d)\n\n        # Refine prompt if required\n        if self.refine_prompt or self.glyph_prompt:\n            self.prompt = self._refine_user_prompt(self.prompt)"}, {"role": "assistant", "content": "\"\"\"\n        Prepares the query, including prompt modifications and image handling.\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Validates and selects the model based on user input or defaults.\"\"\""}, {"role": "assistant", "content": "def _validate_model_selection(self, input_model, valid_models):\n        \n        self.model = (\n            input_model.lower()\n            if input_model.lower() in valid_models\n            else valid_models[0]\n        )"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _validate_model_selection(self, input_model, valid_models):\n        \n        self.model = (\n            input_model.lower()\n            if input_model.lower() in valid_models\n            else valid_models[0]\n        )"}, {"role": "assistant", "content": "\"\"\"Validates and selects the model based on user input or defaults.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Prepares system role text.\"\"\""}, {"role": "assistant", "content": "def _prepare_system_role(self, input_role):\n        \n        roleDict = text_library[\"roles\"]\n        # Selects the role based on user input or defaults.\n        if input_role.lower() in roleDict:\n            self.label = input_role.lower()\n            builtin = roleDict[input_role.lower()]\n            self.role = builtin[\"prompt\"]\n            self.role_name = builtin[\"name\"]\n        elif input_role.lower() in [\"user\", \"\"]:\n            self.role = \"user\"\n            self.label = \"default\"\n            self.role_name = \"Default User\"\n        elif len(self.role.split()) >= 200:\n            self.role_name = \"User-defined role\"\n            self.label = \"custom\"\n        else:\n            self.role_name, self.role = self._refine_custom_role(input_role)\n            self.label = \"custom\"\n\n        # Add chain of thought reporting\n        if self.chain_of_thought:\n            self.role += text_library[\"modifiers\"][\"cot\"]"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _prepare_system_role(self, input_role):\n        \n        roleDict = text_library[\"roles\"]\n        # Selects the role based on user input or defaults.\n        if input_role.lower() in roleDict:\n            self.label = input_role.lower()\n            builtin = roleDict[input_role.lower()]\n            self.role = builtin[\"prompt\"]\n            self.role_name = builtin[\"name\"]\n        elif input_role.lower() in [\"user\", \"\"]:\n            self.role = \"user\"\n            self.label = \"default\"\n            self.role_name = \"Default User\"\n        elif len(self.role.split()) >= 200:\n            self.role_name = \"User-defined role\"\n            self.label = \"custom\"\n        else:\n            self.role_name, self.role = self._refine_custom_role(input_role)\n            self.label = \"custom\"\n\n        # Add chain of thought reporting\n        if self.chain_of_thought:\n            self.role += text_library[\"modifiers\"][\"cot\"]"}, {"role": "assistant", "content": "\"\"\"Prepares system role text.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Reformat input custom user roles for improved outcomes.\"\"\""}, {"role": "assistant", "content": "def _refine_custom_role(self, init_role):\n        \n        self._log_and_print(\n            f\"Refining custom role text...\\n\", self.verbose, self.logging\n        )\n\n        # Reformat role text\n        refine_prompt = (\n            \"Format and improve the following system role propmt to maximize clarity and potential output quality:\\n\\n\"\n            + init_role\n        )\n        custom_role = self._init_chat_completion(refine_prompt)\n\n        # Name custom role\n        refine_prompt = \"Generate a short and accurate name for the following system role prompt:\\n\\n\"\n        refine_prompt += custom_role\n        refine_prompt += \"\\n\\nReturn only the generated name.\"\n        role_name = self._init_chat_completion(refine_prompt)\n\n        reportStr = f\"\"\"Role name: {role_name}\\n\"\"\"\n        reportStr += f\"\"\"\\nDescription:\\n{custom_role}\\n\"\"\"\n\n        self._log_and_print(reportStr, self.verbose, self.logging)\n\n        return role_name, custom_role"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _refine_custom_role(self, init_role):\n        \n        self._log_and_print(\n            f\"Refining custom role text...\\n\", self.verbose, self.logging\n        )\n\n        # Reformat role text\n        refine_prompt = (\n            \"Format and improve the following system role propmt to maximize clarity and potential output quality:\\n\\n\"\n            + init_role\n        )\n        custom_role = self._init_chat_completion(refine_prompt)\n\n        # Name custom role\n        refine_prompt = \"Generate a short and accurate name for the following system role prompt:\\n\\n\"\n        refine_prompt += custom_role\n        refine_prompt += \"\\n\\nReturn only the generated name.\"\n        role_name = self._init_chat_completion(refine_prompt)\n\n        reportStr = f\"\"\"Role name: {role_name}\\n\"\"\"\n        reportStr += f\"\"\"\\nDescription:\\n{custom_role}\\n\"\"\"\n\n        self._log_and_print(reportStr, self.verbose, self.logging)\n\n        return role_name, custom_role"}, {"role": "assistant", "content": "\"\"\"Reformat input custom user roles for improved outcomes.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Generate status message.\"\"\""}, {"role": "assistant", "content": "def status(self):\n        \n        statusStr = f\"\"\"\nAgent parameters:\n    Model: {self.model}\n    Role: {self.role_name}\n    \n    Chain-of-thought: {self.chain_of_thought}\n    Prompt refinement: {self.refine_prompt}\n    Associative glyphs: {self.glyph_prompt}\n    Response iterations: {self.iterations}\n    Subdirectory scanning: {self.scan_dirs}\n    Text logging: {self.logging}\n    Verbose StdOut: {self.verbose}\n    Code snippet detection: {self.save_code}\n\n    Image dimensions: {self.dimensions}\n    Image quality: {self.quality}\n\n    Time stamp: {self.timestamp}\n    Seed: {self.seed}\n    Assistant ID: {self.agent_id}\n    Active chat ID: {self.chat_id}\n    Requests in current chat: {chat.current_chat_messages}\n\"\"\"\n        self._log_and_print(statusStr, True, self.logging)\n\n        # Token usage report\n        self.token_report()\n        # $$$ report\n        self.cost_report()\n        # Chat report\n        self.chat_report()"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def status(self):\n        \n        statusStr = f\"\"\"\nAgent parameters:\n    Model: {self.model}\n    Role: {self.role_name}\n    \n    Chain-of-thought: {self.chain_of_thought}\n    Prompt refinement: {self.refine_prompt}\n    Associative glyphs: {self.glyph_prompt}\n    Response iterations: {self.iterations}\n    Subdirectory scanning: {self.scan_dirs}\n    Text logging: {self.logging}\n    Verbose StdOut: {self.verbose}\n    Code snippet detection: {self.save_code}\n\n    Image dimensions: {self.dimensions}\n    Image quality: {self.quality}\n\n    Time stamp: {self.timestamp}\n    Seed: {self.seed}\n    Assistant ID: {self.agent_id}\n    Active chat ID: {self.chat_id}\n    Requests in current chat: {chat.current_chat_messages}\n\"\"\"\n        self._log_and_print(statusStr, True, self.logging)\n\n        # Token usage report\n        self.token_report()\n        # $$$ report\n        self.cost_report()\n        # Chat report\n        self.chat_report()"}, {"role": "assistant", "content": "\"\"\"Generate status message.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Submits the query to OpenAIs API and processes the response.\"\"\""}, {"role": "assistant", "content": "def chat(self, prompt=\"\"):\n        \n        # Checks for last system response is not prompt provided\n        if prompt == \"\":\n            try:\n                prompt = self.last_message\n            except Exception as e:\n                raise ValueError(f\"No existing messages found in chat: {e}\")\n\n        # Update user prompt\n        self._prepare_query_text(prompt)\n        self._log_and_print(\n            f\"\\n{self.role_name} using {self.model} to process continued conversation...\\n\",\n            True,\n            self.logging,\n        )\n\n        if self.stage != \"refine_only\":\n            if \"dall-e\" not in self.model:\n                chat.current_chat_messages += 1\n                self._handle_text_request()\n            else:\n                self._handle_image_request()\n\n        # Check current scope chat\n        if chat.current_chat_messages >= chat.message_limit:\n            self._log_and_print(\n                f\"\\nReached end of current chat limit.\\n\", self.verbose, False\n            )\n            summary = self.summarize_current_chat()\n            self.start_new_chat(\n                \"The following is a summary of a ongoing conversation with a user and an AI assistant:\\n\"\n                + summary\n            )"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def chat(self, prompt=\"\"):\n        \n        # Checks for last system response is not prompt provided\n        if prompt == \"\":\n            try:\n                prompt = self.last_message\n            except Exception as e:\n                raise ValueError(f\"No existing messages found in chat: {e}\")\n\n        # Update user prompt\n        self._prepare_query_text(prompt)\n        self._log_and_print(\n            f\"\\n{self.role_name} using {self.model} to process continued conversation...\\n\",\n            True,\n            self.logging,\n        )\n\n        if self.stage != \"refine_only\":\n            if \"dall-e\" not in self.model:\n                chat.current_chat_messages += 1\n                self._handle_text_request()\n            else:\n                self._handle_image_request()\n\n        # Check current scope chat\n        if chat.current_chat_messages >= chat.message_limit:\n            self._log_and_print(\n                f\"\\nReached end of current chat limit.\\n\", self.verbose, False\n            )\n            summary = self.summarize_current_chat()\n            self.start_new_chat(\n                \"The following is a summary of a ongoing conversation with a user and an AI assistant:\\n\"\n                + summary\n            )"}, {"role": "assistant", "content": "\"\"\"Submits the query to OpenAIs API and processes the response.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Logs and prints the provided message if verbose.\"\"\""}, {"role": "assistant", "content": "def _log_and_print(self, message, verbose=True, logging=True):\n        \n        if message:\n            if verbose == True and self.silent == False:\n                print(message)\n            if logging == True:\n                with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n                    f.write(message + \"\\n\")"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _log_and_print(self, message, verbose=True, logging=True):\n        \n        if message:\n            if verbose == True and self.silent == False:\n                print(message)\n            if logging == True:\n                with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n                    f.write(message + \"\\n\")"}, {"role": "assistant", "content": "\"\"\"Logs and prints the provided message if verbose.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Summarize current conversation history for future context parsing.\"\"\""}, {"role": "assistant", "content": "def summarize_current_chat(self):\n        \n        self._log_and_print(\n            f\"Using {self.small_model} to summarize current chat...\\n\",\n            self.verbose,\n            False,\n        )\n\n        # Get all chat messages\n        all_messages = self._get_current_messages()\n\n        # Generate concise summary\n        summary_prompt = text_library[\"modifiers\"][\"summarize\"] + \"\\n\\n\" + all_messages\n        summarized = self._init_chat_completion(\n            model=self.small_model,\n            prompt=summary_prompt,\n            iters=self.iterations,\n            seed=self.seed,\n        )\n\n        return summarized"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def summarize_current_chat(self):\n        \n        self._log_and_print(\n            f\"Using {self.small_model} to summarize current chat...\\n\",\n            self.verbose,\n            False,\n        )\n\n        # Get all chat messages\n        all_messages = self._get_current_messages()\n\n        # Generate concise summary\n        summary_prompt = text_library[\"modifiers\"][\"summarize\"] + \"\\n\\n\" + all_messages\n        summarized = self._init_chat_completion(\n            model=self.small_model,\n            prompt=summary_prompt,\n            iters=self.iterations,\n            seed=self.seed,\n        )\n\n        return summarized"}, {"role": "assistant", "content": "\"\"\"Summarize current conversation history for future context parsing.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Processes text-based responses from OpenAIs chat models.\"\"\""}, {"role": "assistant", "content": "def _handle_text_request(self):\n        \n        self.last_message = self._send_chat_message()\n        self._update_token_count(self.run_status)\n        self._calculate_cost()\n        self._log_and_print(self.last_message, True, self.logging)\n\n        # Extract code snippets\n        code_snippets = utils.extract_code_snippets(self.last_message)\n        if self.save_code and len(code_snippets) > 0:\n            self.code_files = []\n            reportStr = \"\\nExtracted code saved to:\\n\"\n            for lang in code_snippets.keys():\n                code = code_snippets[lang]\n                objects = utils.extract_object_names(code, lang)\n                file_name = f\"{utils.find_max_lines(code, objects)}.{self.timestamp}{text_library[\"extensions\"].get(lang, f'.{lang}')}\".lstrip(\n                    \"_.\"\n                )\n                file_name = _check_unique_filename(file_name)\n                reportStr += f\"\\t{file_name}\\n\"\n                self._write_script(code, file_name)\n\n            self._log_and_print(reportStr, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _handle_text_request(self):\n        \n        self.last_message = self._send_chat_message()\n        self._update_token_count(self.run_status)\n        self._calculate_cost()\n        self._log_and_print(self.last_message, True, self.logging)\n\n        # Extract code snippets\n        code_snippets = utils.extract_code_snippets(self.last_message)\n        if self.save_code and len(code_snippets) > 0:\n            self.code_files = []\n            reportStr = \"\\nExtracted code saved to:\\n\"\n            for lang in code_snippets.keys():\n                code = code_snippets[lang]\n                objects = utils.extract_object_names(code, lang)\n                file_name = f\"{utils.find_max_lines(code, objects)}.{self.timestamp}{text_library[\"extensions\"].get(lang, f'.{lang}')}\".lstrip(\n                    \"_.\"\n                )\n                file_name = _check_unique_filename(file_name)\n                reportStr += f\"\\t{file_name}\\n\"\n                self._write_script(code, file_name)\n\n            self._log_and_print(reportStr, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Processes text-based responses from OpenAIs chat models.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Writes code to a file.\"\"\""}, {"role": "assistant", "content": "def _write_script(self, content, file_name, outDir=\"code\", lang=None):\n        \n        os.makedirs(outDir, exist_ok=True)\n        self.code_files.append(f\"{os.getcwd()}/{outDir}/{file_name}\")\n        with open(f\"{outDir}/{file_name}\", \"w\", encoding=\"utf-8\") as f:\n            if lang:\n                f.write(f\"#!/usr/bin/env {lang}\\n\\n\")\n            f.write(f\"# Code generated by {self.model}\\n\\n\")\n            f.write(content)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _write_script(self, content, file_name, outDir=\"code\", lang=None):\n        \n        os.makedirs(outDir, exist_ok=True)\n        self.code_files.append(f\"{os.getcwd()}/{outDir}/{file_name}\")\n        with open(f\"{outDir}/{file_name}\", \"w\", encoding=\"utf-8\") as f:\n            if lang:\n                f.write(f\"#!/usr/bin/env {lang}\\n\\n\")\n            f.write(f\"# Code generated by {self.model}\\n\\n\")\n            f.write(content)"}, {"role": "assistant", "content": "\"\"\"Writes code to a file.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Generates session token report.\"\"\""}, {"role": "assistant", "content": "def token_report(self):\n        \n        allTokensStr = \"\"\n        for x in total_tokens.keys():\n            allTokensStr += f\"{x}: Input = {total_tokens[x]['prompt']}; Completion = {total_tokens[x]['completion']}\\n\"\n\n        tokenStr = \"Overall session tokens:\\n\\t\" + allTokensStr\n        tokenStr += \"\\tCurrent agent tokens:\\n\"\n        tokenStr += f\"\\tInput: {self.tokens['prompt']}\\n\"\n        tokenStr += f\"\\tOutput: {self.tokens['completion']}\\n\"\n\n        self._log_and_print(tokenStr, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def token_report(self):\n        \n        allTokensStr = \"\"\n        for x in total_tokens.keys():\n            allTokensStr += f\"{x}: Input = {total_tokens[x]['prompt']}; Completion = {total_tokens[x]['completion']}\\n\"\n\n        tokenStr = \"Overall session tokens:\\n\\t\" + allTokensStr\n        tokenStr += \"\\tCurrent agent tokens:\\n\"\n        tokenStr += f\"\\tInput: {self.tokens['prompt']}\\n\"\n        tokenStr += f\"\\tOutput: {self.tokens['completion']}\\n\"\n\n        self._log_and_print(tokenStr, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Generates session token report.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Report active chats from current session\"\"\""}, {"role": "assistant", "content": "def chat_report(self):\n        \n        chatStr = \"Current session chat IDs:\\n\" + \"\\n\\t\".join(client.chat_ids)\n        self._log_and_print(chatStr, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def chat_report(self):\n        \n        chatStr = \"Current session chat IDs:\\n\" + \"\\n\\t\".join(client.chat_ids)\n        self._log_and_print(chatStr, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Report active chats from current session\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Calculates approximate cost (USD) of LLM tokens generated to a given decimal place\"\"\""}, {"role": "assistant", "content": "def _calculate_cost(self, rates, dec=5):\n        \n        global total_cost\n        if self.model in rates:\n            prompt_rate, completion_rate = rates.get(self.model)\n            prompt_cost = round((self.tokens[\"prompt\"] * prompt_rate) / 1e6, dec)\n            completion_cost = round(\n                (self.tokens[\"completion\"] * completion_rate) / 1e6, dec\n            )\n        else:\n            prompt_cost = completion_cost = 0.0\n\n        total_cost += round(prompt_cost + completion_cost, dec)\n        self.cost[\"prompt\"] += prompt_cost\n        self.cost[\"completion\"] += completion_cost"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _calculate_cost(self, rates, dec=5):\n        \n        global total_cost\n        if self.model in rates:\n            prompt_rate, completion_rate = rates.get(self.model)\n            prompt_cost = round((self.tokens[\"prompt\"] * prompt_rate) / 1e6, dec)\n            completion_cost = round(\n                (self.tokens[\"completion\"] * completion_rate) / 1e6, dec\n            )\n        else:\n            prompt_cost = completion_cost = 0.0\n\n        total_cost += round(prompt_cost + completion_cost, dec)\n        self.cost[\"prompt\"] += prompt_cost\n        self.cost[\"completion\"] += completion_cost"}, {"role": "assistant", "content": "\"\"\"Calculates approximate cost (USD) of LLM tokens generated to a given decimal place\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Generates session cost report.\"\"\""}, {"role": "assistant", "content": "def cost_report(self, dec=5):\n        \n        costStr = f\"\"\"Overall session cost: ${round(total_cost, dec)}\n\n    Current agent using: {self.model}\n        Subtotal: ${round(self.cost['prompt'] + self.cost['completion'], dec)}\n        Input: ${self.cost['prompt']}\n        Output: ${self.cost['completion']}\n\"\"\"\n        self._log_and_print(costStr, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def cost_report(self, dec=5):\n        \n        costStr = f\"\"\"Overall session cost: ${round(total_cost, dec)}\n\n    Current agent using: {self.model}\n        Subtotal: ${round(self.cost['prompt'] + self.cost['completion'], dec)}\n        Input: ${self.cost['prompt']}\n        Output: ${self.cost['completion']}\n\"\"\"\n        self._log_and_print(costStr, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Generates session cost report.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Condenses multiple API responses into a single coherent response.\"\"\""}, {"role": "assistant", "content": "def _condense_iterations(self, responses):\n        \n        responses = \"\\n\\n\".join(\n            [\n                \"\\n\".join([f\"Iteration: {i + 1}\", responses[i]])\n                for i in range(len(responses))\n            ]\n        )\n\n        self._log_and_print(\n            f\"Using {self.small_model} to condense system responses...\\n\",\n            self.verbose,\n            self.logging,\n        )\n        condensed = self._init_chat_completion(\n            model=self.small_model,\n            prompt=text_library[\"modifiers\"][\"condense\"] + \"\\n\\n\" + responses,\n        )\n\n        self._log_and_print(\n            f\"Condensed text:\\n{condensed}\\n\", self.verbose, self.logging\n        )\n\n        return condensed"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _condense_iterations(self, responses):\n        \n        responses = \"\\n\\n\".join(\n            [\n                \"\\n\".join([f\"Iteration: {i + 1}\", responses[i]])\n                for i in range(len(responses))\n            ]\n        )\n\n        self._log_and_print(\n            f\"Using {self.small_model} to condense system responses...\\n\",\n            self.verbose,\n            self.logging,\n        )\n        condensed = self._init_chat_completion(\n            model=self.small_model,\n            prompt=text_library[\"modifiers\"][\"condense\"] + \"\\n\\n\" + responses,\n        )\n\n        self._log_and_print(\n            f\"Condensed text:\\n{condensed}\\n\", self.verbose, self.logging\n        )\n\n        return condensed"}, {"role": "assistant", "content": "\"\"\"Condenses multiple API responses into a single coherent response.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Refines an LLM prompt using specified rewrite actions.\"\"\""}, {"role": "assistant", "content": "def _refine_user_prompt(self, old_prompt):\n        \n        modifierDict = text_library[\"modifiers\"]\n        refineDict = text_library[\"refinement\"]\n        self._log_and_print(\n            f\"Using {self.small_model} to optimize initial user request...\\n\",\n            True,\n            self.logging,\n        )\n        updated_prompt = old_prompt\n        if self.refine_prompt == True:\n            actions = set([\"expand\", \"amplify\"])\n            actions |= set(\n                re.sub(r\"[^\\w\\s]\", \"\", word).lower()\n                for word in old_prompt.split()\n                if word.lower() in refineDict\n            )\n            action_str = \"\\n\".join(refineDict[a] for a in actions) + \"\\n\\n\"\n            updated_prompt = modifierDict[\"refine\"] + action_str + old_prompt\n\n        if self.glyph_prompt == True:\n            updated_prompt += modifierDict[\"glyph\"]\n\n        refined = self._init_chat_completion(\n            model=self.small_model, prompt=updated_prompt\n        )\n\n        new_prompt = (\n            self._condense_iterations(refined) if self.iterations > 1 else refined\n        )\n\n        self._log_and_print(\n            f\"Refined query prompt:\\n{new_prompt}\", self.verbose, self.logging\n        )\n\n        return new_prompt"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _refine_user_prompt(self, old_prompt):\n        \n        modifierDict = text_library[\"modifiers\"]\n        refineDict = text_library[\"refinement\"]\n        self._log_and_print(\n            f\"Using {self.small_model} to optimize initial user request...\\n\",\n            True,\n            self.logging,\n        )\n        updated_prompt = old_prompt\n        if self.refine_prompt == True:\n            actions = set([\"expand\", \"amplify\"])\n            actions |= set(\n                re.sub(r\"[^\\w\\s]\", \"\", word).lower()\n                for word in old_prompt.split()\n                if word.lower() in refineDict\n            )\n            action_str = \"\\n\".join(refineDict[a] for a in actions) + \"\\n\\n\"\n            updated_prompt = modifierDict[\"refine\"] + action_str + old_prompt\n\n        if self.glyph_prompt == True:\n            updated_prompt += modifierDict[\"glyph\"]\n\n        refined = self._init_chat_completion(\n            model=self.small_model, prompt=updated_prompt\n        )\n\n        new_prompt = (\n            self._condense_iterations(refined) if self.iterations > 1 else refined\n        )\n\n        self._log_and_print(\n            f\"Refined query prompt:\\n{new_prompt}\", self.verbose, self.logging\n        )\n\n        return new_prompt"}, {"role": "assistant", "content": "\"\"\"Refines an LLM prompt using specified rewrite actions.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n    Prepare logging setup.\n    \"\"\""}, {"role": "assistant", "content": "def setup_logging(prefix):\n    \n    log_file = utils.check_unique_filename(f\"logs/{prefix}.transcript.log\")\n    os.makedirs(\"logs\", exist_ok=True)\n    with open(log_file, \"w\") as f:\n        f.write(\"New session initiated.\\n\")\n\n    return log_file"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def setup_logging(prefix):\n    \n    log_file = utils.check_unique_filename(f\"logs/{prefix}.transcript.log\")\n    os.makedirs(\"logs\", exist_ok=True)\n    with open(log_file, \"w\") as f:\n        f.write(\"New session initiated.\\n\")\n\n    return log_file"}, {"role": "assistant", "content": "\"\"\"\n    Prepare logging setup.\n    \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Create a binary-like variable from a string for use a random seed\"\"\""}, {"role": "assistant", "content": "def string_to_binary(input_string):\n    \n    # Convert all characters in a str to ASCII values and then to 8-bit binary\n    binary = \"\".join([format(ord(char), \"08b\") for char in input_string])\n\n    return int(binary[0 : len(str(sys.maxsize))])  # Constrain length"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def string_to_binary(input_string):\n    \n    # Convert all characters in a str to ASCII values and then to 8-bit binary\n    binary = \"\".join([format(ord(char), \"08b\") for char in input_string])\n\n    return int(binary[0 : len(str(sys.maxsize))])  # Constrain length"}, {"role": "assistant", "content": "\"\"\"Create a binary-like variable from a string for use a random seed\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Check if a file has a code extension.\"\"\""}, {"role": "assistant", "content": "def _is_code_file(file_path):\n    \n    return os.path.splitext(file_path)[1].lower() in set(extDict.values())"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _is_code_file(file_path):\n    \n    return os.path.splitext(file_path)[1].lower() in set(extDict.values())"}, {"role": "assistant", "content": "\"\"\"Check if a file has a code extension.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n    Extract defined object names (functions, classes, and variables) from a code snippet.\n    \"\"\""}, {"role": "assistant", "content": "def extract_object_names(code, language):\n    \n    # Get language-specific patterns\n    patterns = patternDict.get(language, {})\n\n    # Extract object names using the language-specific patterns\n    classes = patterns.get(\"class\", re.compile(r\"\")).findall(code)\n    functions = patterns.get(\"function\", re.compile(r\"\")).findall(code)\n    variables = patterns.get(\"variable\", re.compile(r\"\")).findall(code)\n\n    # Select objects to return based on hierarachy\n    if len(classes) > 0:\n        return classes\n    elif len(functions) > 0:\n        return functions\n    else:\n        return variables"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def extract_object_names(code, language):\n    \n    # Get language-specific patterns\n    patterns = patternDict.get(language, {})\n\n    # Extract object names using the language-specific patterns\n    classes = patterns.get(\"class\", re.compile(r\"\")).findall(code)\n    functions = patterns.get(\"function\", re.compile(r\"\")).findall(code)\n    variables = patterns.get(\"variable\", re.compile(r\"\")).findall(code)\n\n    # Select objects to return based on hierarachy\n    if len(classes) > 0:\n        return classes\n    elif len(functions) > 0:\n        return functions\n    else:\n        return variables"}, {"role": "assistant", "content": "\"\"\"\n    Extract defined object names (functions, classes, and variables) from a code snippet.\n    \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n    Count the number of lines of code for each object in the code snippet.\n\n    Args:\n        code (str): The code snippet to analyze.\n        object_names (list): A list of object names to count lines for.\n\n    Returns:\n        str: Name of object with the largest line count.\n    \"\"\""}, {"role": "assistant", "content": "def find_max_lines(code, object_names):\n    \n    rm_names = [\"main\", \"functions\", \"classes\", \"variables\"]\n    line_counts = {name: 0 for name in object_names if name not in rm_names}\n    line_counts[\"code\"] = 1\n    current_object = None\n\n    for line in code.split(\"\\n\"):\n        # Check if the line defines a new object\n        for name in object_names:\n            if re.match(rf\"\\s*(def|class)\\s+{name}\\s*[\\(:]\", line):\n                current_object = name\n                break\n\n        # Count lines for the current object\n        if current_object and line.strip() and current_object not in rm_names:\n            line_counts[current_object] += 1\n\n    return max(line_counts, key=line_counts.get)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def find_max_lines(code, object_names):\n    \n    rm_names = [\"main\", \"functions\", \"classes\", \"variables\"]\n    line_counts = {name: 0 for name in object_names if name not in rm_names}\n    line_counts[\"code\"] = 1\n    current_object = None\n\n    for line in code.split(\"\\n\"):\n        # Check if the line defines a new object\n        for name in object_names:\n            if re.match(rf\"\\s*(def|class)\\s+{name}\\s*[\\(:]\", line):\n                current_object = name\n                break\n\n        # Count lines for the current object\n        if current_object and line.strip() and current_object not in rm_names:\n            line_counts[current_object] += 1\n\n    return max(line_counts, key=line_counts.get)"}, {"role": "assistant", "content": "\"\"\"\n    Count the number of lines of code for each object in the code snippet.\n\n    Args:\n        code (str): The code snippet to analyze.\n        object_names (list): A list of object names to count lines for.\n\n    Returns:\n        str: Name of object with the largest line count.\n    \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Ensure temperature and top_p are valid\"\"\""}, {"role": "assistant", "content": "def validate_probability_params(temp, topp):\n    \n    temp = 0.7 if not (0.0 <= temp <= 2.0) else temp\n    topp = 1.0 if not (0.0 <= topp <= 2.0) or temp != 0.7 else topp\n    return temp, topp"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def validate_probability_params(temp, topp):\n    \n    temp = 0.7 if not (0.0 <= temp <= 2.0) else temp\n    topp = 1.0 if not (0.0 <= topp <= 2.0) or temp != 0.7 else topp\n    return temp, topp"}, {"role": "assistant", "content": "\"\"\"Ensure temperature and top_p are valid\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Reads the contents of a given file.\"\"\""}, {"role": "assistant", "content": "def read_file_contents(filename):\n    \n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        return f\"# File: {filename}\\n{f.read()}\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def read_file_contents(filename):\n    \n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        return f\"# File: {filename}\\n{f.read()}\""}, {"role": "assistant", "content": "\"\"\"Reads the contents of a given file.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Recursively scan a directory and return the content of all code files.\"\"\""}, {"role": "assistant", "content": "def scan_directory(path=\"code\"):\n    \n    codebase = \"\"\n    for root, _, files in os.walk(path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if _is_code_file(file_path):\n                codebase += f\"File: {file_path}\\n\"\n                codebase += read_file_contents(file_path) + \"\\n\\n\"\n\n    return codebase"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def scan_directory(path=\"code\"):\n    \n    codebase = \"\"\n    for root, _, files in os.walk(path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            if _is_code_file(file_path):\n                codebase += f\"File: {file_path}\\n\"\n                codebase += read_file_contents(file_path) + \"\\n\\n\"\n\n    return codebase"}, {"role": "assistant", "content": "\"\"\"Recursively scan a directory and return the content of all code files.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Scan the input string for existing paths and return them in separate lists.\"\"\""}, {"role": "assistant", "content": "def find_existing_paths(prompt):\n    \n    # Regular expression to match potential file paths\n    path_pattern = re.compile(r'([a-zA-Z]:\\\\[^:<>\"|?\\n]*|/[^:<>\"|?\\n]*)')\n\n    # Find all matches in the input string\n    matches = path_pattern.findall(prompt)\n\n    # Separate files and directories\n    existing_paths = []\n    for match in matches:\n        if os.path.isdir(match):\n            existing_paths.append(match)\n\n    return existing_paths"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def find_existing_paths(prompt):\n    \n    # Regular expression to match potential file paths\n    path_pattern = re.compile(r'([a-zA-Z]:\\\\[^:<>\"|?\\n]*|/[^:<>\"|?\\n]*)')\n\n    # Find all matches in the input string\n    matches = path_pattern.findall(prompt)\n\n    # Separate files and directories\n    existing_paths = []\n    for match in matches:\n        if os.path.isdir(match):\n            existing_paths.append(match)\n\n    return existing_paths"}, {"role": "assistant", "content": "\"\"\"Scan the input string for existing paths and return them in separate lists.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Filter filenames by checking if they exist in the current directory or system's PATH\"\"\""}, {"role": "assistant", "content": "def find_existing_files(prompt):\n    \n    existing_files = [\n        x for x in prompt.split() if os.path.isfile(x.rstrip(string.punctuation))\n    ]\n    return existing_files"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def find_existing_files(prompt):\n    \n    existing_files = [\n        x for x in prompt.split() if os.path.isfile(x.rstrip(string.punctuation))\n    ]\n    return existing_files"}, {"role": "assistant", "content": "\"\"\"Filter filenames by checking if they exist in the current directory or system's PATH\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Extract code snippets from a large body of text using triple backticks as delimiters.\"\"\""}, {"role": "assistant", "content": "def extract_code_snippets(message):\n    \n    # Regular expression to match code blocks enclosed in triple backticks, including the language tag\n    code_snippets = defaultdict(str)\n    code_pattern = re.compile(r\"```(\\w+)\\n(.*?)```\", re.DOTALL)\n    snippets = code_pattern.findall(message)\n    for lang, code in snippets:\n        code_snippets[lang] += code.strip()\n\n    return code_snippets"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def extract_code_snippets(message):\n    \n    # Regular expression to match code blocks enclosed in triple backticks, including the language tag\n    code_snippets = defaultdict(str)\n    code_pattern = re.compile(r\"```(\\w+)\\n(.*?)```\", re.DOTALL)\n    snippets = code_pattern.findall(message)\n    for lang, code in snippets:\n        code_snippets[lang] += code.strip()\n\n    return code_snippets"}, {"role": "assistant", "content": "\"\"\"Extract code snippets from a large body of text using triple backticks as delimiters.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Evaluates the query and determines which agent from the available agent roles is best suited\n        to handle the query.\n\n        Args:\n            query (str): The user's query/prompt to be evaluated\n\n        Returns:\n            Agent: The most suitable agent to handle the query\n\n        Raises:\n            ValueError: If no agents are available or if no suitable agent is found\n        \"\"\""}, {"role": "assistant", "content": "def select_best_agent(self, query: str) -> Agent:\n        \n        if not self._agents:\n            raise ValueError(\"No agents available for query evaluation\")\n\n        # Create a prompt to evaluate which agent role best matches the query\n        evaluation_prompt = f\"\"\"\n        Given the following query: \"{query}\"\n        \n        And these available agent roles:\n        {[agent.role.description for agent in self._agents]}\n        \n        Determine which role is best suited to handle this query. Consider the expertise and \n        responsibilities of each role. Return only the index number (0-{len(self._agents) - 1}) \n        of the most suitable role.\"\"\"\n\n        # Use the coach's LLM client to evaluate the best agent\n        response = self.client.complete(evaluation_prompt)\n\n        try:\n            best_agent_index = int(response.strip())\n            if 0 <= best_agent_index < len(self._agents):\n                return self._agents[best_agent_index]\n            raise ValueError(\"Invalid agent index returned by LLM\")\n        except (ValueError, IndexError):\n            # Fallback to the first agent if parsing fails\n            return self._agents[0]"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def select_best_agent(self, query: str) -> Agent:\n        \n        if not self._agents:\n            raise ValueError(\"No agents available for query evaluation\")\n\n        # Create a prompt to evaluate which agent role best matches the query\n        evaluation_prompt = f\"\"\"\n        Given the following query: \"{query}\"\n        \n        And these available agent roles:\n        {[agent.role.description for agent in self._agents]}\n        \n        Determine which role is best suited to handle this query. Consider the expertise and \n        responsibilities of each role. Return only the index number (0-{len(self._agents) - 1}) \n        of the most suitable role.\"\"\"\n\n        # Use the coach's LLM client to evaluate the best agent\n        response = self.client.complete(evaluation_prompt)\n\n        try:\n            best_agent_index = int(response.strip())\n            if 0 <= best_agent_index < len(self._agents):\n                return self._agents[best_agent_index]\n            raise ValueError(\"Invalid agent index returned by LLM\")\n        except (ValueError, IndexError):\n            # Fallback to the first agent if parsing fails\n            return self._agents[0]"}, {"role": "assistant", "content": "\"\"\"\n        Evaluates the query and determines which agent from the available agent roles is best suited\n        to handle the query.\n\n        Args:\n            query (str): The user's query/prompt to be evaluated\n\n        Returns:\n            Agent: The most suitable agent to handle the query\n\n        Raises:\n            ValueError: If no agents are available or if no suitable agent is found\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Process a request through the OpenAI API\n\n        Args:\n            prompt: The input prompt\n            role: System role instructions\n            model: Model to use\n            temperature: Temperature setting\n            top_p: Top p setting\n\n        Returns:\n            Wrapped response object\n        \"\"\""}, {"role": "assistant", "content": "def process_request(\n        self,\n        prompt: str,\n        role: str,\n        model: str = \"gpt-4o-mini\",\n        temperature: float = 0.7,\n        top_p: float = 1.0,\n    ) -> OpenAIResponse:\n        \n        # Create message in thread\n        self.client.beta.threads.messages.create(\n            thread_id=self.thread.id, role=\"user\", content=prompt\n        )\n\n        # Create and run assistant\n        assistant = self.client.beta.assistants.create(\n            name=\"Agent\", instructions=role, model=model\n        )\n\n        run = self.client.beta.threads.runs.create(\n            thread_id=self.thread.id, assistant_id=assistant.id\n        )\n\n        # Wait for completion\n        while True:\n            run_status = self.client.beta.threads.runs.retrieve(\n                thread_id=self.thread.id, run_id=run.id\n            )\n            if run_status.status in [\"completed\", \"failed\"]:\n                break\n\n        # Get response\n        messages = self.client.beta.threads.messages.list(thread_id=self.thread.id)\n\n        if not messages.data:\n            raise RuntimeError(\"No response received from OpenAI\")\n\n        return OpenAIResponse(\n            content=messages.data[0].content[0].text.value, usage=run_status.usage\n        )"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def process_request(\n        self,\n        prompt: str,\n        role: str,\n        model: str = \"gpt-4o-mini\",\n        temperature: float = 0.7,\n        top_p: float = 1.0,\n    ) -> OpenAIResponse:\n        \n        # Create message in thread\n        self.client.beta.threads.messages.create(\n            thread_id=self.thread.id, role=\"user\", content=prompt\n        )\n\n        # Create and run assistant\n        assistant = self.client.beta.assistants.create(\n            name=\"Agent\", instructions=role, model=model\n        )\n\n        run = self.client.beta.threads.runs.create(\n            thread_id=self.thread.id, assistant_id=assistant.id\n        )\n\n        # Wait for completion\n        while True:\n            run_status = self.client.beta.threads.runs.retrieve(\n                thread_id=self.thread.id, run_id=run.id\n            )\n            if run_status.status in [\"completed\", \"failed\"]:\n                break\n\n        # Get response\n        messages = self.client.beta.threads.messages.list(thread_id=self.thread.id)\n\n        if not messages.data:\n            raise RuntimeError(\"No response received from OpenAI\")\n\n        return OpenAIResponse(\n            content=messages.data[0].content[0].text.value, usage=run_status.usage\n        )"}, {"role": "assistant", "content": "\"\"\"\n        Process a request through the OpenAI API\n\n        Args:\n            prompt: The input prompt\n            role: System role instructions\n            model: Model to use\n            temperature: Temperature setting\n            top_p: Top p setting\n\n        Returns:\n            Wrapped response object\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Start a new chat with only the current agent and adds previous context if needed.\"\"\""}, {"role": "assistant", "content": "def start_new_chat(self, context=None):\n        \n        global chat\n        chat = client.beta.threads.create()\n        chat.current_chat_calls = 0\n        chat.message_limit = self.message_limit\n\n        # Add previous context\n        if context:\n            previous_context = client.beta.threads.messages.create(\n                chat_id=chat.id, role=\"user\", content=context\n            )\n\n        global client\n        client.chat_ids |= set([chat.id])\n        self.chat_id = chat.id\n\n        # Report\n        self._log_and_print(\n            f\"New chat created and added to current agent: {self.chat_id}\\n\",\n            self.verbose,\n            self.logging,\n        )"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def start_new_chat(self, context=None):\n        \n        global chat\n        chat = client.beta.threads.create()\n        chat.current_chat_calls = 0\n        chat.message_limit = self.message_limit\n\n        # Add previous context\n        if context:\n            previous_context = client.beta.threads.messages.create(\n                chat_id=chat.id, role=\"user\", content=context\n            )\n\n        global client\n        client.chat_ids |= set([chat.id])\n        self.chat_id = chat.id\n\n        # Report\n        self._log_and_print(\n            f\"New chat created and added to current agent: {self.chat_id}\\n\",\n            self.verbose,\n            self.logging,\n        )"}, {"role": "assistant", "content": "\"\"\"Start a new chat with only the current agent and adds previous context if needed.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Initialize and submit a single chat completion request\"\"\""}, {"role": "assistant", "content": "def _init_chat_completion(self, prompt, model, role=\"user\"):\n        \n        message = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"system\", \"content\": role},\n        ]\n\n        completion = client.chat.completions.create(\n            model=model,\n            messages=message,\n            n=self.iterations,\n            seed=self.seed,\n            temperature=self.temperature,\n            top_p=self.top_p,\n        )\n        self._update_token_count(completion)\n        self._calculate_cost(openai_rates)\n\n        return completion"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _init_chat_completion(self, prompt, model, role=\"user\"):\n        \n        message = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"system\", \"content\": role},\n        ]\n\n        completion = client.chat.completions.create(\n            model=model,\n            messages=message,\n            n=self.iterations,\n            seed=self.seed,\n            temperature=self.temperature,\n            top_p=self.top_p,\n        )\n        self._update_token_count(completion)\n        self._calculate_cost(openai_rates)\n\n        return completion"}, {"role": "assistant", "content": "\"\"\"Initialize and submit a single chat completion request\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Updates token count for prompt and completion.\"\"\""}, {"role": "assistant", "content": "def _update_token_count(self, response_obj):\n        \n        global total_tokens\n        total_tokens[self.model][\"prompt\"] += response_obj.usage.prompt_tokens\n        total_tokens[self.model][\"completion\"] += response_obj.usage.completion_tokens\n        # Agent-specific counts\n        self.tokens[\"prompt\"] += response_obj.usage.prompt_tokens\n        self.tokens[\"completion\"] += response_obj.usage.completion_tokens"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _update_token_count(self, response_obj):\n        \n        global total_tokens\n        total_tokens[self.model][\"prompt\"] += response_obj.usage.prompt_tokens\n        total_tokens[self.model][\"completion\"] += response_obj.usage.completion_tokens\n        # Agent-specific counts\n        self.tokens[\"prompt\"] += response_obj.usage.prompt_tokens\n        self.tokens[\"completion\"] += response_obj.usage.completion_tokens"}, {"role": "assistant", "content": "\"\"\"Updates token count for prompt and completion.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Report active chats from current session\"\"\""}, {"role": "assistant", "content": "def chat_report(self):\n        \n        chatStr = f\"\"\"Current session chats:\n    {'\\n\\t'.join(client.chat_ids)}\n\"\"\"\n        self._log_and_print(chatStr, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def chat_report(self):\n        \n        chatStr = f\"\"\"Current session chats:\n    {'\\n\\t'.join(client.chat_ids)}\n\"\"\"\n        self._log_and_print(chatStr, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Report active chats from current session\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Creates a new assistant based on user-defined parameters\n\n        Args:\n            interpreter (bool): Whether to enable the code interpreter tool.\n\n        Returns:\n            New assistant assistant class instance\n        \"\"\""}, {"role": "assistant", "content": "def _create_new_agent(self, interpreter=False):\n        \n        try:\n            agent = client.beta.assistants.create(\n                name=self.role_name,\n                instructions=self.role,\n                model=self.model,\n                tools=[{\"type\": \"code_interpreter\"}] if interpreter == True else [],\n            )\n            self.agent = agent.id\n        except Exception as e:\n            raise RuntimeError(f\"Failed to create assistant: {e}\")"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _create_new_agent(self, interpreter=False):\n        \n        try:\n            agent = client.beta.assistants.create(\n                name=self.role_name,\n                instructions=self.role,\n                model=self.model,\n                tools=[{\"type\": \"code_interpreter\"}] if interpreter == True else [],\n            )\n            self.agent = agent.id\n        except Exception as e:\n            raise RuntimeError(f\"Failed to create assistant: {e}\")"}, {"role": "assistant", "content": "\"\"\"\n        Creates a new assistant based on user-defined parameters\n\n        Args:\n            interpreter (bool): Whether to enable the code interpreter tool.\n\n        Returns:\n            New assistant assistant class instance\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Fetches all messages from a chat in order and returns them as a text block.\"\"\""}, {"role": "assistant", "content": "def _get_current_messages(self):\n        \n        messages = client.beta.threads.messages.list(chat_id=self.chat_id)\n        sorted_messages = sorted(messages.data, key=lambda msg: msg.created_at)\n        conversation = [x.content[0].text.value.strip() for x in sorted_messages]\n\n        return \"\\n\\n\".join(conversation)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _get_current_messages(self):\n        \n        messages = client.beta.threads.messages.list(chat_id=self.chat_id)\n        sorted_messages = sorted(messages.data, key=lambda msg: msg.created_at)\n        conversation = [x.content[0].text.value.strip() for x in sorted_messages]\n\n        return \"\\n\\n\".join(conversation)"}, {"role": "assistant", "content": "\"\"\"Fetches all messages from a chat in order and returns them as a text block.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Sends a user prompt to an existing chat, runs the assistant,\n        and retrieves the response if successful.\n\n        Returns:\n            str: The text response from the assistant.\n\n        Raises:\n            ValueError: If the assistant fails to generate a response.\n        \"\"\""}, {"role": "assistant", "content": "def _send_chat_message(self) -> str:\n        \n        # Adds user prompt to existing chat.\n        try:\n            new_message = client.beta.threads.messages.create(\n                chat_id=self.chat_id, role=\"user\", content=self.prompt\n            )\n        except Exception as e:\n            raise RuntimeError(f\"Failed to create message: {e}\")\n\n        # Run the assistant on the chat\n        current_run = client.beta.threads.runs.create(\n            chat_id=self.chat_id, assistant_id=self.agent\n        )\n\n        # Wait for completion and retrieve responses\n        while True:\n            self.run_status = client.beta.threads.runs.retrieve(\n                chat_id=self.chat_id, run_id=current_run.id\n            )\n            if self.run_status.status in [\"completed\", \"failed\"]:\n                break\n            else:\n                time.sleep(1)  # Wait before polling again\n\n        if self.run_status.status == \"completed\":\n            messages = client.beta.threads.messages.list(chat_id=self.chat_id)\n            if messages.data:  # Check if messages list is not empty\n                return messages.data[0].content[0].text.value\n            else:\n                raise ValueError(\"No messages found in the chat.\")\n        else:\n            raise ValueError(\"Assistant failed to generate a response.\")"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _send_chat_message(self) -> str:\n        \n        # Adds user prompt to existing chat.\n        try:\n            new_message = client.beta.threads.messages.create(\n                chat_id=self.chat_id, role=\"user\", content=self.prompt\n            )\n        except Exception as e:\n            raise RuntimeError(f\"Failed to create message: {e}\")\n\n        # Run the assistant on the chat\n        current_run = client.beta.threads.runs.create(\n            chat_id=self.chat_id, assistant_id=self.agent\n        )\n\n        # Wait for completion and retrieve responses\n        while True:\n            self.run_status = client.beta.threads.runs.retrieve(\n                chat_id=self.chat_id, run_id=current_run.id\n            )\n            if self.run_status.status in [\"completed\", \"failed\"]:\n                break\n            else:\n                time.sleep(1)  # Wait before polling again\n\n        if self.run_status.status == \"completed\":\n            messages = client.beta.threads.messages.list(chat_id=self.chat_id)\n            if messages.data:  # Check if messages list is not empty\n                return messages.data[0].content[0].text.value\n            else:\n                raise ValueError(\"No messages found in the chat.\")\n        else:\n            raise ValueError(\"Assistant failed to generate a response.\")"}, {"role": "assistant", "content": "\"\"\"\n        Sends a user prompt to an existing chat, runs the assistant,\n        and retrieves the response if successful.\n\n        Returns:\n            str: The text response from the assistant.\n\n        Raises:\n            ValueError: If the assistant fails to generate a response.\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Processes image generation requests using OpenAIs image models.\"\"\""}, {"role": "assistant", "content": "def _handle_image_request(self):\n        \n        os.makedirs(\"images\", exist_ok=True)\n        response = client.images.generate(\n            model=self.model,\n            prompt=self.prompt,\n            n=1,\n            size=self.dimensions,\n            quality=self.quality,\n        )\n        self._update_token_count(response)\n        self._calculate_cost()\n        self._log_and_print(\n            f\"\\nRevised initial prompt:\\n{response.data[0].revised_prompt}\",\n            self.verbose,\n            self.logging,\n        )\n        image_data = requests.get(response.data[0].url).content\n        image_file = f\"images/{self.prefix}.image.png\"\n        image_file = _check_unique_filename(image_file)\n        with open(image_file, \"wb\") as outFile:\n            outFile.write(image_data)\n\n        self.last_message = (\n            \"\\nRevised image prompt:\\n\"\n            + response.data[0].revised_prompt\n            + \"\\nGenerated image saved to:\\n\"\n            + image_file\n        )\n        self._log_and_print(self.last_message, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _handle_image_request(self):\n        \n        os.makedirs(\"images\", exist_ok=True)\n        response = client.images.generate(\n            model=self.model,\n            prompt=self.prompt,\n            n=1,\n            size=self.dimensions,\n            quality=self.quality,\n        )\n        self._update_token_count(response)\n        self._calculate_cost()\n        self._log_and_print(\n            f\"\\nRevised initial prompt:\\n{response.data[0].revised_prompt}\",\n            self.verbose,\n            self.logging,\n        )\n        image_data = requests.get(response.data[0].url).content\n        image_file = f\"images/{self.prefix}.image.png\"\n        image_file = _check_unique_filename(image_file)\n        with open(image_file, \"wb\") as outFile:\n            outFile.write(image_data)\n\n        self.last_message = (\n            \"\\nRevised image prompt:\\n\"\n            + response.data[0].revised_prompt\n            + \"\\nGenerated image saved to:\\n\"\n            + image_file\n        )\n        self._log_and_print(self.last_message, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Processes image generation requests using OpenAIs image models.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Validates image dimensions and quality for the model.\"\"\""}, {"role": "assistant", "content": "def _validate_image_params(self, dimensions, quality):\n        \n        valid_dimensions = {\n            \"dall-e-3\": [\"1024x1024\", \"1792x1024\", \"1024x1792\"],\n            \"dall-e-2\": [\"1024x1024\", \"512x512\", \"256x256\"],\n        }\n        if (\n            self.model in valid_dimensions\n            and dimensions.lower() not in valid_dimensions[self.model]\n        ):\n            self.dimensions = \"1024x1024\"\n        else:\n            self.dimensions = dimensions\n\n        self.quality = (\n            \"hd\"\n            if quality.lower() in {\"h\", \"hd\", \"high\", \"higher\", \"highest\"}\n            else \"standard\"\n        )\n        self.quality = (\n            \"hd\" if self.label == \"photographer\" else self.quality\n        )  # Check for photo role"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _validate_image_params(self, dimensions, quality):\n        \n        valid_dimensions = {\n            \"dall-e-3\": [\"1024x1024\", \"1792x1024\", \"1024x1792\"],\n            \"dall-e-2\": [\"1024x1024\", \"512x512\", \"256x256\"],\n        }\n        if (\n            self.model in valid_dimensions\n            and dimensions.lower() not in valid_dimensions[self.model]\n        ):\n            self.dimensions = \"1024x1024\"\n        else:\n            self.dimensions = dimensions\n\n        self.quality = (\n            \"hd\"\n            if quality.lower() in {\"h\", \"hd\", \"high\", \"higher\", \"highest\"}\n            else \"standard\"\n        )\n        self.quality = (\n            \"hd\" if self.label == \"photographer\" else self.quality\n        )  # Check for photo role"}, {"role": "assistant", "content": "\"\"\"Validates image dimensions and quality for the model.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Initialize and submit a single chat completion request\"\"\""}, {"role": "assistant", "content": "def _init_chat_completion(self, prompt, model):\n        \n        cfg = types.GenerateContentConfig(\n            temperature=self.temperature,\n            topP=self.top_p,\n            seed=self.seed,\n            candidateCount=self.iterations,\n        )\n\n        prompt = [prompt] if isinstance(prompt, str) else prompt\n        completion = client.models.generate_content(\n            model=model, contents=prompt, config=cfg\n        )\n        self._update_token_count(completion)\n        self._calculate_cost(google_rates)\n\n        return completion.text"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _init_chat_completion(self, prompt, model):\n        \n        cfg = types.GenerateContentConfig(\n            temperature=self.temperature,\n            topP=self.top_p,\n            seed=self.seed,\n            candidateCount=self.iterations,\n        )\n\n        prompt = [prompt] if isinstance(prompt, str) else prompt\n        completion = client.models.generate_content(\n            model=model, contents=prompt, config=cfg\n        )\n        self._update_token_count(completion)\n        self._calculate_cost(google_rates)\n\n        return completion.text"}, {"role": "assistant", "content": "\"\"\"Initialize and submit a single chat completion request\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Start a new chat with only the current agent and adds previous context if needed.\"\"\""}, {"role": "assistant", "content": "def start_new_chat(self, context=\"\"):\n        \n        global chat\n        chat = client.chats.create(model=self.model, context=context)\n        chat.current_chat_calls = 0\n        chat.message_limit = self.message_limit\n\n        global client\n        client.chat_ids |= set([chat.id])\n        self.chat_id = chat.id\n        self._log_and_print(\n            f\"New chat created and added to current agent: {self.chat_id}\\n\",\n            self.verbose,\n            self.logging,\n        )"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def start_new_chat(self, context=\"\"):\n        \n        global chat\n        chat = client.chats.create(model=self.model, context=context)\n        chat.current_chat_calls = 0\n        chat.message_limit = self.message_limit\n\n        global client\n        client.chat_ids |= set([chat.id])\n        self.chat_id = chat.id\n        self._log_and_print(\n            f\"New chat created and added to current agent: {self.chat_id}\\n\",\n            self.verbose,\n            self.logging,\n        )"}, {"role": "assistant", "content": "\"\"\"Start a new chat with only the current agent and adds previous context if needed.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Updates token count for prompt and completion.\"\"\""}, {"role": "assistant", "content": "def _update_token_count(self, response_obj):\n        \n        usage = response_obj.usage_metadata\n        global total_tokens\n        total_tokens[self.model][\"prompt\"] += usage[\"prompt_token_count\"]\n        total_tokens[self.model][\"completion\"] += usage[\"candidates_token_count\"]\n        # Agent-specific counts\n        self.tokens[\"prompt\"] += usage[\"prompt_token_count\"]\n        self.tokens[\"completion\"] += usage[\"candidates_token_count\"]"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _update_token_count(self, response_obj):\n        \n        usage = response_obj.usage_metadata\n        global total_tokens\n        total_tokens[self.model][\"prompt\"] += usage[\"prompt_token_count\"]\n        total_tokens[self.model][\"completion\"] += usage[\"candidates_token_count\"]\n        # Agent-specific counts\n        self.tokens[\"prompt\"] += usage[\"prompt_token_count\"]\n        self.tokens[\"completion\"] += usage[\"candidates_token_count\"]"}, {"role": "assistant", "content": "\"\"\"Updates token count for prompt and completion.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Fetches all messages from a chat in order and returns them as a text block.\"\"\""}, {"role": "assistant", "content": "def _get_current_messages(self):\n        \n        conversation = []\n        speaker = \"User:\"\n        for entry in self.chat_history:\n            conversation.append(f\"{speaker}\\n{entry}\")\n            speaker = \"System:\" if speaker == \"User:\" else \"User:\"\n\n        return \"\\n\\n\".join(conversation)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _get_current_messages(self):\n        \n        conversation = []\n        speaker = \"User:\"\n        for entry in self.chat_history:\n            conversation.append(f\"{speaker}\\n{entry}\")\n            speaker = \"System:\" if speaker == \"User:\" else \"User:\"\n\n        return \"\\n\\n\".join(conversation)"}, {"role": "assistant", "content": "\"\"\"Fetches all messages from a chat in order and returns them as a text block.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Sends a user prompt to an existing chat and retrieves the response.\"\"\""}, {"role": "assistant", "content": "def _send_chat_message(self):\n        \n        self.chat_history.append({\"role\": \"user\", \"content\": self.prompt})\n\n        # Send user prompt to existing chat and add to the chat history\n        response = chat.send_message(self.prompt, config=self.agent)\n        self.chat_history.append({\"role\": \"model\", \"content\": response.text})\n\n        return response.text"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _send_chat_message(self):\n        \n        self.chat_history.append({\"role\": \"user\", \"content\": self.prompt})\n\n        # Send user prompt to existing chat and add to the chat history\n        response = chat.send_message(self.prompt, config=self.agent)\n        self.chat_history.append({\"role\": \"model\", \"content\": response.text})\n\n        return response.text"}, {"role": "assistant", "content": "\"\"\"Sends a user prompt to an existing chat and retrieves the response.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Creates a new assistant config based on user-defined parameters.\"\"\""}, {"role": "assistant", "content": "def _create_agent_configuration(self):\n    \n    self.agent = types.GenerateContentConfig(\n        system_instruction=self.role,\n        temperature=self.temperature,\n        topP=self.top_p,\n        seed=self.seed,\n        candidateCount=self.iterations,\n        tools=[\n            types.Tool(\n                google_search=types.GoogleSearchRetrieval(\n                    dynamic_retrieval_config=types.DynamicRetrievalConfig(\n                        dynamic_threshold=self.threshold\n                    )\n                )\n            )\n        ],\n    )\n\n    # Requires even more testing\n    def _validate_image_params(self, dimensions, quality):\n        \"\"\"Validates image dimensions and quality for the Gemini model.\"\"\"\n        valid_dimensions = {\n            \"gemini-pro\": [\"1024x1024\", \"768x768\", \"512x512\"],  # Example dimensions\n            \"gemini-ultra\": [\"2048x2048\", \"1024x1024\", \"512x512\"],  # Example dimensions\n        }\n\n        # Validate dimensions\n        if (\n            self.model in valid_dimensions\n            and dimensions.lower() not in valid_dimensions[self.model]\n        ):\n            self.dimensions = \"1024x1024\"  # Default dimension\n        else:\n            self.dimensions = dimensions\n\n        # Validate quality\n        self.quality = (\n            \"high\"\n            if quality.lower() in {\"h\", \"hd\", \"high\", \"higher\", \"highest\"}\n            else \"standard\"\n        )\n\n        # Override quality for specific roles (e.g., photographer)\n        if self.label == \"photographer\":\n            self.quality = \"high\"\n\n        return self.dimensions, self.quality\n\n    # Still needs refactoring\n    def _handle_image_request(self):\n        \"\"\"Processes image generation requests using OpenAIs image models.\"\"\"\n        os.makedirs(\"images\", exist_ok=True)\n\n        response = model.generate_image(\n            prompt=self.prompt,\n            dimensions=self.dimensions,\n            quality=self.quality,\n        )\n\n        response = client.images.generate(\n            model=self.model,\n            prompt=self.prompt,\n            n=1,\n            size=self.dimensions,\n            quality=self.quality,\n        )\n        self._update_token_count(response)\n        self._calculate_cost()\n        self._log_and_print(\n            f\"\\nRevised initial prompt:\\n{response.data[0].revised_prompt}\",\n            self.verbose,\n            self.logging,\n        )\n        image_data = requests.get(response.data[0].url).content\n        image_file = f\"images/{self.prefix}.image.png\"\n        image_file = _check_unique_filename(image_file)\n        with open(image_file, \"wb\") as outFile:\n            outFile.write(image_data)\n\n        self.last_message = (\n            \"\\nRevised image prompt:\\n\"\n            + response.data[0].revised_prompt\n            + \"\\nGenerated image saved to:\\n\"\n            + image_file\n        )\n        self._log_and_print(self.last_message, True, self.logging)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _create_agent_configuration(self):\n    \n    self.agent = types.GenerateContentConfig(\n        system_instruction=self.role,\n        temperature=self.temperature,\n        topP=self.top_p,\n        seed=self.seed,\n        candidateCount=self.iterations,\n        tools=[\n            types.Tool(\n                google_search=types.GoogleSearchRetrieval(\n                    dynamic_retrieval_config=types.DynamicRetrievalConfig(\n                        dynamic_threshold=self.threshold\n                    )\n                )\n            )\n        ],\n    )\n\n    # Requires even more testing\n    def _validate_image_params(self, dimensions, quality):\n        \"\"\"Validates image dimensions and quality for the Gemini model.\"\"\"\n        valid_dimensions = {\n            \"gemini-pro\": [\"1024x1024\", \"768x768\", \"512x512\"],  # Example dimensions\n            \"gemini-ultra\": [\"2048x2048\", \"1024x1024\", \"512x512\"],  # Example dimensions\n        }\n\n        # Validate dimensions\n        if (\n            self.model in valid_dimensions\n            and dimensions.lower() not in valid_dimensions[self.model]\n        ):\n            self.dimensions = \"1024x1024\"  # Default dimension\n        else:\n            self.dimensions = dimensions\n\n        # Validate quality\n        self.quality = (\n            \"high\"\n            if quality.lower() in {\"h\", \"hd\", \"high\", \"higher\", \"highest\"}\n            else \"standard\"\n        )\n\n        # Override quality for specific roles (e.g., photographer)\n        if self.label == \"photographer\":\n            self.quality = \"high\"\n\n        return self.dimensions, self.quality\n\n    # Still needs refactoring\n    def _handle_image_request(self):\n        \"\"\"Processes image generation requests using OpenAIs image models.\"\"\"\n        os.makedirs(\"images\", exist_ok=True)\n\n        response = model.generate_image(\n            prompt=self.prompt,\n            dimensions=self.dimensions,\n            quality=self.quality,\n        )\n\n        response = client.images.generate(\n            model=self.model,\n            prompt=self.prompt,\n            n=1,\n            size=self.dimensions,\n            quality=self.quality,\n        )\n        self._update_token_count(response)\n        self._calculate_cost()\n        self._log_and_print(\n            f\"\\nRevised initial prompt:\\n{response.data[0].revised_prompt}\",\n            self.verbose,\n            self.logging,\n        )\n        image_data = requests.get(response.data[0].url).content\n        image_file = f\"images/{self.prefix}.image.png\"\n        image_file = _check_unique_filename(image_file)\n        with open(image_file, \"wb\") as outFile:\n            outFile.write(image_data)\n\n        self.last_message = (\n            \"\\nRevised image prompt:\\n\"\n            + response.data[0].revised_prompt\n            + \"\\nGenerated image saved to:\\n\"\n            + image_file\n        )\n        self._log_and_print(self.last_message, True, self.logging)"}, {"role": "assistant", "content": "\"\"\"Creates a new assistant config based on user-defined parameters.\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Display the UI interface\"\"\""}, {"role": "assistant", "content": "def display(self):\n        \n        display(self.interface)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def display(self):\n        \n        display(self.interface)"}, {"role": "assistant", "content": "\"\"\"Display the UI interface\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Callback for adding a predefined agent\"\"\""}, {"role": "assistant", "content": "def _add_agent_cb(self, b):\n        \n        agent_name = self.agent_dropdown.value\n        if agent_name not in self._selected_agent_names:\n            self._selected_agent_names.append(agent_name)\n            agent_widget = SelectedAgentWidget(agent_name, self._remove_agent)\n            self.selected_agents.children = list(self.selected_agents.children) + [\n                agent_widget.interface\n            ]\n\n            # Remove from dropdown options\n            new_options = [\n                opt for opt in self.agent_dropdown.options if opt != agent_name\n            ]\n            self.agent_dropdown.options = new_options\n            if new_options:\n                self.agent_dropdown.value = new_options[0]"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _add_agent_cb(self, b):\n        \n        agent_name = self.agent_dropdown.value\n        if agent_name not in self._selected_agent_names:\n            self._selected_agent_names.append(agent_name)\n            agent_widget = SelectedAgentWidget(agent_name, self._remove_agent)\n            self.selected_agents.children = list(self.selected_agents.children) + [\n                agent_widget.interface\n            ]\n\n            # Remove from dropdown options\n            new_options = [\n                opt for opt in self.agent_dropdown.options if opt != agent_name\n            ]\n            self.agent_dropdown.options = new_options\n            if new_options:\n                self.agent_dropdown.value = new_options[0]"}, {"role": "assistant", "content": "\"\"\"Callback for adding a predefined agent\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Callback for adding a custom agent\"\"\""}, {"role": "assistant", "content": "def _add_custom_agent_cb(self, b):\n        \n        name = self.custom_agent_name.value\n        if name and name not in self._selected_agent_names:\n            self._selected_agent_names.append(name)\n            agent_widget = SelectedAgentWidget(name, self._remove_agent)\n            self.selected_agents.children = list(self.selected_agents.children) + [\n                agent_widget.interface\n            ]\n\n            # Clear inputs\n            self.custom_agent_name.value = \"\"\n            self.custom_agent_prompt.value = \"\"\n            self.refine_prompt_toggle.value = \"No\"\n            self.glyph_refinement_check.value = False\n            self.chain_of_though_refinement_check.value = False"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _add_custom_agent_cb(self, b):\n        \n        name = self.custom_agent_name.value\n        if name and name not in self._selected_agent_names:\n            self._selected_agent_names.append(name)\n            agent_widget = SelectedAgentWidget(name, self._remove_agent)\n            self.selected_agents.children = list(self.selected_agents.children) + [\n                agent_widget.interface\n            ]\n\n            # Clear inputs\n            self.custom_agent_name.value = \"\"\n            self.custom_agent_prompt.value = \"\"\n            self.refine_prompt_toggle.value = \"No\"\n            self.glyph_refinement_check.value = False\n            self.chain_of_though_refinement_check.value = False"}, {"role": "assistant", "content": "\"\"\"Callback for adding a custom agent\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Remove an agent from the selected agents\"\"\""}, {"role": "assistant", "content": "def _remove_agent(self, agent_name: str):\n        \n        if agent_name in self._selected_agent_names:\n            self._selected_agent_names.remove(agent_name)\n\n            # Update selected agents display\n            new_children = [\n                child\n                for child in self.selected_agents.children\n                if agent_name not in str(child.children[0].value)\n            ]\n            self.selected_agents.children = new_children\n\n            # Add back to dropdown if it was a predefined agent\n            if agent_name in [\n                \"artist\",\n                \"assistant\",\n                \"data_scientist\",\n                \"data_visualization\",\n                \"developer\",\n                \"editor\",\n                \"image\",\n                \"photographer\",\n                \"prompt_engineer\",\n                \"refactor\",\n                \"unit_test\",\n                \"writer\",\n            ]:\n                self.agent_dropdown.options = sorted(\n                    list(self.agent_dropdown.options) + [agent_name]\n                )"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _remove_agent(self, agent_name: str):\n        \n        if agent_name in self._selected_agent_names:\n            self._selected_agent_names.remove(agent_name)\n\n            # Update selected agents display\n            new_children = [\n                child\n                for child in self.selected_agents.children\n                if agent_name not in str(child.children[0].value)\n            ]\n            self.selected_agents.children = new_children\n\n            # Add back to dropdown if it was a predefined agent\n            if agent_name in [\n                \"artist\",\n                \"assistant\",\n                \"data_scientist\",\n                \"data_visualization\",\n                \"developer\",\n                \"editor\",\n                \"image\",\n                \"photographer\",\n                \"prompt_engineer\",\n                \"refactor\",\n                \"unit_test\",\n                \"writer\",\n            ]:\n                self.agent_dropdown.options = sorted(\n                    list(self.agent_dropdown.options) + [agent_name]\n                )"}, {"role": "assistant", "content": "\"\"\"Remove an agent from the selected agents\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Reset the UI to initial state\"\"\""}, {"role": "assistant", "content": "def _reset_cb(self, b):\n        \n        self._selected_agent_names = []\n        self.selected_agents.children = []\n        self.agent_dropdown.options = [\n            \"artist\",\n            \"assistant\",\n            \"data_scientist\",\n            \"data_visualization\",\n            \"developer\",\n            \"editor\",\n            \"image\",\n            \"photographer\",\n            \"prompt_engineer\",\n            \"refactor\",\n            \"unit_test\",\n            \"writer\",\n        ]\n        self.agent_dropdown.value = \"assistant\"\n        self.model_selection.value = \"gpt-4o-mini\"\n        self.write_code_to_files_check.value = False\n        self.log_level.value = \"INFO\"\n        self.custom_agent_name.value = \"\"\n        self.custom_agent_prompt.value = \"\"\n        self.refine_prompt_toggle.value = \"No\"\n        self.glyph_refinement_check.value = False\n        self.chain_of_though_refinement_check.value = False\n        self.token_display.value = \"Total Tokens: 0\"\n        self.cost_display.value = \"Total Cost: $0.00\"\n        self.log_display.value = \"\"\n        self.status_message.value = \"Status: No team created\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _reset_cb(self, b):\n        \n        self._selected_agent_names = []\n        self.selected_agents.children = []\n        self.agent_dropdown.options = [\n            \"artist\",\n            \"assistant\",\n            \"data_scientist\",\n            \"data_visualization\",\n            \"developer\",\n            \"editor\",\n            \"image\",\n            \"photographer\",\n            \"prompt_engineer\",\n            \"refactor\",\n            \"unit_test\",\n            \"writer\",\n        ]\n        self.agent_dropdown.value = \"assistant\"\n        self.model_selection.value = \"gpt-4o-mini\"\n        self.write_code_to_files_check.value = False\n        self.log_level.value = \"INFO\"\n        self.custom_agent_name.value = \"\"\n        self.custom_agent_prompt.value = \"\"\n        self.refine_prompt_toggle.value = \"No\"\n        self.glyph_refinement_check.value = False\n        self.chain_of_though_refinement_check.value = False\n        self.token_display.value = \"Total Tokens: 0\"\n        self.cost_display.value = \"Total Cost: $0.00\"\n        self.log_display.value = \"\"\n        self.status_message.value = \"Status: No team created\""}, {"role": "assistant", "content": "\"\"\"Reset the UI to initial state\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Configure and create a new agent\n\n        Args:\n            config: Configuration settings for the agent\n            role_name: Name of the role to assign to agent (defaults to assistant)\n\n        Returns:\n            Configured Agent instance\n        \"\"\""}, {"role": "assistant", "content": "def create_agent(\n        self,\n        role: str = \"assistant\",\n        config: AgentConfig = DEFAULT_CONFIG,\n        client: Client = OpenAIClient,\n    ) -> Agent:\n        \n\n        # Get role class or raise error if role not found\n        role_name = role.lower()\n        if role_name not in self._roles:\n            available_roles = \", \".join(sorted(self._roles.keys()))\n            raise ValueError(\n                f\"Unknown role '{role_name}'. Available roles are: {available_roles}\"\n            )\n\n        role_class = self._roles[role_name]\n        role = role_class()\n\n        # Create and return configured agent\n        return Agent(config=config, role=role, client=client)"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def create_agent(\n        self,\n        role: str = \"assistant\",\n        config: AgentConfig = DEFAULT_CONFIG,\n        client: Client = OpenAIClient,\n    ) -> Agent:\n        \n\n        # Get role class or raise error if role not found\n        role_name = role.lower()\n        if role_name not in self._roles:\n            available_roles = \", \".join(sorted(self._roles.keys()))\n            raise ValueError(\n                f\"Unknown role '{role_name}'. Available roles are: {available_roles}\"\n            )\n\n        role_class = self._roles[role_name]\n        role = role_class()\n\n        # Create and return configured agent\n        return Agent(config=config, role=role, client=client)"}, {"role": "assistant", "content": "\"\"\"\n        Configure and create a new agent\n\n        Args:\n            config: Configuration settings for the agent\n            role_name: Name of the role to assign to agent (defaults to assistant)\n\n        Returns:\n            Configured Agent instance\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Register a new role to the factory.\n\n        Args:\n            new_role: The role instance to register\n\n        Raises:\n            ValueError: If role with same name already exists\n        \"\"\""}, {"role": "assistant", "content": "def add_role(self, new_role: AgentRole) -> None:\n        \n        role_name = new_role.name.lower()\n        if role_name in self._roles:\n            raise ValueError(f\"Role '{role_name}' already exists\")\n\n        self._roles[role_name] = new_role.__class__"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def add_role(self, new_role: AgentRole) -> None:\n        \n        role_name = new_role.name.lower()\n        if role_name in self._roles:\n            raise ValueError(f\"Role '{role_name}' already exists\")\n\n        self._roles[role_name] = new_role.__class__"}, {"role": "assistant", "content": "\"\"\"\n        Register a new role to the factory.\n\n        Args:\n            new_role: The role instance to register\n\n        Raises:\n            ValueError: If role with same name already exists\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"\n        Process a request using the configured client\n\n        Args:\n            prompt: The input prompt to process\n\n        Returns:\n            The processed response\n        \"\"\""}, {"role": "assistant", "content": "def request(self, prompt: str = \"\") -> str:\n        \n        # Use last message if no prompt provided\n        if not prompt and self.last_message:\n            prompt = self.last_message\n        elif not prompt:\n            raise ValueError(\"No prompt provided and no existing messages found\")\n\n        # Prepare the prompt\n        prepared_prompt = self._prepare_prompt(prompt)\n\n        # Process request through client\n        response = self.client.process_request(\n            prompt=prepared_prompt,\n            role=self.role.prompt,\n            model=self.config.model,\n            temperature=self.config.temperature,\n            top_p=self.config.top_p,\n        )\n\n        # Update tracking\n        self._update_usage(response)\n        self.last_message = response.content\n\n        # Log if enabled\n        if self.config.logging:\n            self._log_interaction(prompt, response.content)\n\n        return response.content"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def request(self, prompt: str = \"\") -> str:\n        \n        # Use last message if no prompt provided\n        if not prompt and self.last_message:\n            prompt = self.last_message\n        elif not prompt:\n            raise ValueError(\"No prompt provided and no existing messages found\")\n\n        # Prepare the prompt\n        prepared_prompt = self._prepare_prompt(prompt)\n\n        # Process request through client\n        response = self.client.process_request(\n            prompt=prepared_prompt,\n            role=self.role.prompt,\n            model=self.config.model,\n            temperature=self.config.temperature,\n            top_p=self.config.top_p,\n        )\n\n        # Update tracking\n        self._update_usage(response)\n        self.last_message = response.content\n\n        # Log if enabled\n        if self.config.logging:\n            self._log_interaction(prompt, response.content)\n\n        return response.content"}, {"role": "assistant", "content": "\"\"\"\n        Process a request using the configured client\n\n        Args:\n            prompt: The input prompt to process\n\n        Returns:\n            The processed response\n        \"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Prepare the prompt with any configured modifications\"\"\""}, {"role": "assistant", "content": "def _prepare_prompt(self, prompt: str) -> str:\n        \n        if self.config.use_chain_of_thought:\n            # Add chain of thought instructions\n            prompt += \"\\n\\nUse chain of thought reasoning to solve this problem.\"\n\n        if self.config.use_glyph_promts:\n            # Add glyph formatting instructions\n            prompt += \"\\n\\nFormat response using glyph structure.\"\n\n        return prompt"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _prepare_prompt(self, prompt: str) -> str:\n        \n        if self.config.use_chain_of_thought:\n            # Add chain of thought instructions\n            prompt += \"\\n\\nUse chain of thought reasoning to solve this problem.\"\n\n        if self.config.use_glyph_promts:\n            # Add glyph formatting instructions\n            prompt += \"\\n\\nFormat response using glyph structure.\"\n\n        return prompt"}, {"role": "assistant", "content": "\"\"\"Prepare the prompt with any configured modifications\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Initialize logging setup\"\"\""}, {"role": "assistant", "content": "def _setup_logging(self):\n        \n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.log_file = f\"logs/agent_{timestamp}.log\"\n        os.makedirs(\"logs\", exist_ok=True)\n\n        with open(self.log_file, \"w\") as f:\n            f.write(f\"Agent initialized with role: {self.role.name}\\n\")"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _setup_logging(self):\n        \n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.log_file = f\"logs/agent_{timestamp}.log\"\n        os.makedirs(\"logs\", exist_ok=True)\n\n        with open(self.log_file, \"w\") as f:\n            f.write(f\"Agent initialized with role: {self.role.name}\\n\")"}, {"role": "assistant", "content": "\"\"\"Initialize logging setup\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Log an interaction to the log file\"\"\""}, {"role": "assistant", "content": "def _log_interaction(self, prompt: str, response: str):\n        \n        with open(self.log_file, \"a\") as f:\n            f.write(f\"\\nPrompt: {prompt}\\n\")\n            f.write(f\"Response: {response}\\n\")\n            f.write(\"-\" * 80 + \"\\n\")"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _log_interaction(self, prompt: str, response: str):\n        \n        with open(self.log_file, \"a\") as f:\n            f.write(f\"\\nPrompt: {prompt}\\n\")\n            f.write(f\"Response: {response}\\n\")\n            f.write(\"-\" * 80 + \"\\n\")"}, {"role": "assistant", "content": "\"\"\"Log an interaction to the log file\"\"\""}]}
{"messages": [{"role": "system", "content": "You are a python expert. Write an implementation for the following description."}, {"role": "user", "content": "\"\"\"Update token and cost tracking\"\"\""}, {"role": "assistant", "content": "def _update_usage(self, response: Any):\n        \n        # Update tokens\n        self.tokens[\"prompt\"] += response.usage.prompt_tokens\n        self.tokens[\"completion\"] += response.usage.completion_tokens\n\n        # Calculate costs (simplified example rates)\n        self.cost[\"prompt\"] += response.usage.prompt_tokens * 0.0001\n        self.cost[\"completion\"] += response.usage.completion_tokens * 0.0002"}]}
{"messages": [{"role": "system", "content": "You are a python expert. Explain the following code."}, {"role": "user", "content": "def _update_usage(self, response: Any):\n        \n        # Update tokens\n        self.tokens[\"prompt\"] += response.usage.prompt_tokens\n        self.tokens[\"completion\"] += response.usage.completion_tokens\n\n        # Calculate costs (simplified example rates)\n        self.cost[\"prompt\"] += response.usage.prompt_tokens * 0.0001\n        self.cost[\"completion\"] += response.usage.completion_tokens * 0.0002"}, {"role": "assistant", "content": "\"\"\"Update token and cost tracking\"\"\""}]}
